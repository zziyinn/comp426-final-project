import type { BaseLLMParams } from "@langchain/core/language_models/llms";
import type { BaseChatModelCallOptions, BindToolsInput } from "@langchain/core/language_models/chat_models";
import { BaseMessage, BaseMessageChunk, MessageContent } from "@langchain/core/messages";
import { ChatGenerationChunk, ChatResult } from "@langchain/core/outputs";
import type { JsonStream } from "./utils/stream.js";
import { MediaManager } from "./experimental/utils/media_core.js";
import { AnthropicResponseData, AnthropicAPIConfig } from "./types-anthropic.js";
export * from "./types-anthropic.js";
/**
 * Parameters needed to setup the client connection.
 * AuthOptions are something like GoogleAuthOptions (from google-auth-library)
 * or WebGoogleAuthOptions.
 */
export interface GoogleClientParams<AuthOptions> {
    authOptions?: AuthOptions;
    /** Some APIs allow an API key instead */
    apiKey?: string;
}
/**
 * What platform is this running on?
 * gai - Google AI Studio / MakerSuite / Generative AI platform
 * gcp - Google Cloud Platform
 */
export type GooglePlatformType = "gai" | "gcp";
export interface GoogleConnectionParams<AuthOptions> extends GoogleClientParams<AuthOptions> {
    /** Hostname for the API call (if this is running on GCP) */
    endpoint?: string;
    /** Region where the LLM is stored (if this is running on GCP) */
    location?: string;
    /** The version of the API functions. Part of the path. */
    apiVersion?: string;
    /**
     * What platform to run the service on.
     * If not specified, the class should determine this from other
     * means. Either way, the platform actually used will be in
     * the "platform" getter.
     */
    platformType?: GooglePlatformType;
}
export declare const GoogleAISafetyCategory: {
    readonly Harassment: "HARM_CATEGORY_HARASSMENT";
    readonly HARASSMENT: "HARM_CATEGORY_HARASSMENT";
    readonly HARM_CATEGORY_HARASSMENT: "HARM_CATEGORY_HARASSMENT";
    readonly HateSpeech: "HARM_CATEGORY_HATE_SPEECH";
    readonly HATE_SPEECH: "HARM_CATEGORY_HATE_SPEECH";
    readonly HARM_CATEGORY_HATE_SPEECH: "HARM_CATEGORY_HATE_SPEECH";
    readonly SexuallyExplicit: "HARM_CATEGORY_SEXUALLY_EXPLICIT";
    readonly SEXUALLY_EXPLICIT: "HARM_CATEGORY_SEXUALLY_EXPLICIT";
    readonly HARM_CATEGORY_SEXUALLY_EXPLICIT: "HARM_CATEGORY_SEXUALLY_EXPLICIT";
    readonly Dangerous: "HARM_CATEGORY_DANGEROUS";
    readonly DANGEROUS: "HARM_CATEGORY_DANGEROUS";
    readonly HARM_CATEGORY_DANGEROUS: "HARM_CATEGORY_DANGEROUS";
    readonly CivicIntegrity: "HARM_CATEGORY_CIVIC_INTEGRITY";
    readonly CIVIC_INTEGRITY: "HARM_CATEGORY_CIVIC_INTEGRITY";
    readonly HARM_CATEGORY_CIVIC_INTEGRITY: "HARM_CATEGORY_CIVIC_INTEGRITY";
};
export type GoogleAISafetyCategory = (typeof GoogleAISafetyCategory)[keyof typeof GoogleAISafetyCategory];
export declare const GoogleAISafetyThreshold: {
    readonly None: "BLOCK_NONE";
    readonly NONE: "BLOCK_NONE";
    readonly BLOCK_NONE: "BLOCK_NONE";
    readonly Few: "BLOCK_ONLY_HIGH";
    readonly FEW: "BLOCK_ONLY_HIGH";
    readonly BLOCK_ONLY_HIGH: "BLOCK_ONLY_HIGH";
    readonly Some: "BLOCK_MEDIUM_AND_ABOVE";
    readonly SOME: "BLOCK_MEDIUM_AND_ABOVE";
    readonly BLOCK_MEDIUM_AND_ABOVE: "BLOCK_MEDIUM_AND_ABOVE";
    readonly Most: "BLOCK_LOW_AND_ABOVE";
    readonly MOST: "BLOCK_LOW_AND_ABOVE";
    readonly BLOCK_LOW_AND_ABOVE: "BLOCK_LOW_AND_ABOVE";
    readonly Off: "OFF";
    readonly OFF: "OFF";
    readonly BLOCK_OFF: "OFF";
};
export type GoogleAISafetyThreshold = (typeof GoogleAISafetyThreshold)[keyof typeof GoogleAISafetyThreshold];
export declare const GoogleAISafetyMethod: {
    readonly Severity: "SEVERITY";
    readonly Probability: "PROBABILITY";
};
export type GoogleAISafetyMethod = (typeof GoogleAISafetyMethod)[keyof typeof GoogleAISafetyMethod];
export interface GoogleAISafetySetting {
    category: GoogleAISafetyCategory | string;
    threshold: GoogleAISafetyThreshold | string;
    method?: GoogleAISafetyMethod | string;
}
export type GoogleAIResponseMimeType = "text/plain" | "application/json";
export interface GoogleAIModelParams {
    /** Model to use */
    model?: string;
    /**
     * Model to use
     * Alias for `model`
     */
    modelName?: string;
    /** Sampling temperature to use */
    temperature?: number;
    /**
     * Maximum number of tokens to generate in the completion.
     */
    maxOutputTokens?: number;
    /**
     * Top-p changes how the model selects tokens for output.
     *
     * Tokens are selected from most probable to least until the sum
     * of their probabilities equals the top-p value.
     *
     * For example, if tokens A, B, and C have a probability of
     * .3, .2, and .1 and the top-p value is .5, then the model will
     * select either A or B as the next token (using temperature).
     */
    topP?: number;
    /**
     * Top-k changes how the model selects tokens for output.
     *
     * A top-k of 1 means the selected token is the most probable among
     * all tokens in the modelâ€™s vocabulary (also called greedy decoding),
     * while a top-k of 3 means that the next token is selected from
     * among the 3 most probable tokens (using temperature).
     */
    topK?: number;
    stopSequences?: string[];
    safetySettings?: GoogleAISafetySetting[];
    convertSystemMessageToHumanContent?: boolean;
    /**
     * Available for `gemini-1.5-pro`.
     * The output format of the generated candidate text.
     * Supported MIME types:
     *  - `text/plain`: Text output.
     *  - `application/json`: JSON response in the candidates.
     *
     * @default "text/plain"
     */
    responseMimeType?: GoogleAIResponseMimeType;
    /**
     * Whether or not to stream.
     * @default false
     */
    streaming?: boolean;
}
export type GoogleAIToolType = BindToolsInput | GeminiTool;
/**
 * The params which can be passed to the API at request time.
 */
export interface GoogleAIModelRequestParams extends GoogleAIModelParams {
    tools?: GoogleAIToolType[];
    /**
     * Force the model to use tools in a specific way.
     *
     * | Mode     |	Description                                                                                                                                             |
     * |----------|---------------------------------------------------------------------------------------------------------------------------------------------------------|
     * | "auto"	  | The default model behavior. The model decides whether to predict a function call or a natural language response.                                        |
     * | "any"	  | The model must predict only function calls. To limit the model to a subset of functions, define the allowed function names in `allowed_function_names`. |
     * | "none"	  | The model must not predict function calls. This behavior is equivalent to a model request without any associated function declarations.                 |
     * | string   | The string value must be one of the function names. This will force the model to predict the specified function call.                                   |
     *
     * The tool configuration's "any" mode ("forced function calling") is supported for Gemini 1.5 Pro models only.
     */
    tool_choice?: string | "auto" | "any" | "none" | Record<string, any>;
    /**
     * Allowed functions to call when the mode is "any".
     * If empty, any one of the provided functions are called.
     */
    allowed_function_names?: string[];
}
export interface GoogleAIBaseLLMInput<AuthOptions> extends BaseLLMParams, GoogleConnectionParams<AuthOptions>, GoogleAIModelParams, GoogleAISafetyParams, GoogleAIAPIParams {
}
export interface GoogleAIBaseLanguageModelCallOptions extends BaseChatModelCallOptions, GoogleAIModelRequestParams, GoogleAISafetyParams {
    /**
     * Whether or not to include usage data, like token counts
     * in the streamed response chunks.
     * @default true
     */
    streamUsage?: boolean;
}
/**
 * Input to LLM class.
 */
export interface GoogleBaseLLMInput<AuthOptions> extends GoogleAIBaseLLMInput<AuthOptions> {
}
export interface GoogleResponse {
    data: any;
}
export interface GoogleRawResponse extends GoogleResponse {
    data: Blob;
}
export interface GeminiPartText {
    text: string;
}
export interface GeminiPartInlineData {
    inlineData: {
        mimeType: string;
        data: string;
    };
}
export interface GeminiPartFileData {
    fileData: {
        mimeType: string;
        fileUri: string;
    };
}
export interface GeminiPartFunctionCall {
    functionCall: {
        name: string;
        args?: object;
    };
}
export interface GeminiPartFunctionResponse {
    functionResponse: {
        name: string;
        response: object;
    };
}
export type GeminiPart = GeminiPartText | GeminiPartInlineData | GeminiPartFileData | GeminiPartFunctionCall | GeminiPartFunctionResponse;
export interface GeminiSafetySetting {
    category: string;
    threshold: string;
}
export type GeminiSafetyRating = {
    category: string;
    probability: string;
} & Record<string, unknown>;
export type GeminiRole = "system" | "user" | "model" | "function";
export interface GeminiContent {
    parts: GeminiPart[];
    role: GeminiRole;
}
export interface GeminiTool {
    functionDeclarations?: GeminiFunctionDeclaration[];
}
export interface GeminiFunctionDeclaration {
    name: string;
    description: string;
    parameters?: GeminiFunctionSchema;
}
export interface GeminiFunctionSchema {
    type: GeminiFunctionSchemaType;
    format?: string;
    description?: string;
    nullable?: boolean;
    enum?: string[];
    properties?: Record<string, GeminiFunctionSchema>;
    required?: string[];
    items?: GeminiFunctionSchema;
}
export type GeminiFunctionSchemaType = "string" | "number" | "integer" | "boolean" | "array" | "object";
export interface GeminiGenerationConfig {
    stopSequences?: string[];
    candidateCount?: number;
    maxOutputTokens?: number;
    temperature?: number;
    topP?: number;
    topK?: number;
    responseMimeType?: GoogleAIResponseMimeType;
}
export interface GeminiRequest {
    contents?: GeminiContent[];
    systemInstruction?: GeminiContent;
    tools?: GeminiTool[];
    toolConfig?: {
        functionCallingConfig: {
            mode: "auto" | "any" | "none";
            allowedFunctionNames?: string[];
        };
    };
    safetySettings?: GeminiSafetySetting[];
    generationConfig?: GeminiGenerationConfig;
}
interface GeminiResponseCandidate {
    content: {
        parts: GeminiPart[];
        role: string;
    };
    finishReason: string;
    index: number;
    tokenCount?: number;
    safetyRatings: GeminiSafetyRating[];
}
interface GeminiResponsePromptFeedback {
    blockReason?: string;
    safetyRatings: GeminiSafetyRating[];
}
export interface GenerateContentResponseData {
    candidates: GeminiResponseCandidate[];
    promptFeedback: GeminiResponsePromptFeedback;
    usageMetadata: Record<string, unknown>;
}
export type GoogleLLMModelFamily = null | "palm" | "gemini";
export type VertexModelFamily = GoogleLLMModelFamily | "claude";
export type GoogleLLMResponseData = JsonStream | GenerateContentResponseData | GenerateContentResponseData[];
export interface GoogleLLMResponse extends GoogleResponse {
    data: GoogleLLMResponseData | AnthropicResponseData;
}
export interface GoogleAISafetyHandler {
    /**
     * A function that will take a response and return the, possibly modified,
     * response or throw an exception if there are safety issues.
     *
     * @throws GoogleAISafetyError
     */
    handle(response: GoogleLLMResponse): GoogleLLMResponse;
}
export interface GoogleAISafetyParams {
    safetyHandler?: GoogleAISafetyHandler;
}
export type GeminiJsonSchema = Record<string, unknown> & {
    properties?: Record<string, GeminiJsonSchema>;
    type: GeminiFunctionSchemaType;
};
export interface GeminiJsonSchemaDirty extends GeminiJsonSchema {
    items?: GeminiJsonSchemaDirty;
    properties?: Record<string, GeminiJsonSchemaDirty>;
    additionalProperties?: boolean;
}
export type GoogleAIAPI = {
    messageContentToParts?: (content: MessageContent) => Promise<GeminiPart[]>;
    baseMessageToContent?: (message: BaseMessage, prevMessage: BaseMessage | undefined, useSystemInstruction: boolean) => Promise<GeminiContent[]>;
    responseToString: (response: GoogleLLMResponse) => string;
    responseToChatGeneration: (response: GoogleLLMResponse) => ChatGenerationChunk | null;
    chunkToString: (chunk: BaseMessageChunk) => string;
    responseToBaseMessage: (response: GoogleLLMResponse) => BaseMessage;
    responseToChatResult: (response: GoogleLLMResponse) => ChatResult;
    formatData: (input: unknown, parameters: GoogleAIModelRequestParams) => Promise<unknown>;
};
export interface GeminiAPIConfig {
    safetyHandler?: GoogleAISafetyHandler;
    mediaManager?: MediaManager;
    useSystemInstruction?: boolean;
}
export type GoogleAIAPIConfig = GeminiAPIConfig | AnthropicAPIConfig;
export interface GoogleAIAPIParams {
    apiName?: string;
    apiConfig?: GoogleAIAPIConfig;
}
